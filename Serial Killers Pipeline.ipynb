{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías que nos ayudarán a correr el código. \n",
    "\n",
    "from bs4 import BeautifulSoup #librería para hacer parse de documentos HTML\n",
    "import requests #librería para hacer los requests a la página que se va a scrappear \n",
    "import pandas as pd #librería para análisis de datos \n",
    "import re #librería para limpieza de la información \n",
    "import datanews #API \n",
    "import urllib.parse #librería para hacer el encoding de los url \n",
    "import json #librería para parsing\n",
    "import time #librería para dar un intervalo de tiempo dentro de un loop\n",
    "import numpy as np #librería para trabajar con arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap():\n",
    "    \"\"\"Function to scrap serial killers with the highest known victim count from Wikipedia\"\"\"\n",
    "\n",
    "#Dar la instrucción de scrappear el contenido de la página de Wikipedia donde vienen la lista de los asesinos\n",
    "#seriales rankeados por número de víctimas. Para ordenar la información y que sea más legible usamos Beautiful Soup. \n",
    "\n",
    "    wiki = requests.get(\"https://en.wikipedia.org/wiki/List_of_serial_killers_by_number_of_victims\").content\n",
    "    soup = BeautifulSoup(wiki, 'html5lib')\n",
    "\n",
    "#De todo el contenido de la página, solo me interesa la tabla donde viene agrupada la info que necesito \n",
    "#Con Inspect identifiqué que la class que necesito llamar es \"wikitable sortable\"\n",
    "\n",
    "    killers_info = soup.select('table[class=\"wikitable sortable\"]')\n",
    "\n",
    "#La información seleccionada es limpiada. \n",
    "\n",
    "    clean_killers_info = [th.text.strip().replace(\"\\xa0\",\",\").split('\\n\\n\\n') for th in killers_info]\n",
    "\n",
    "#De la clase que identificamos, solo necesitamos la primera tabla que corresponde a los asesinos seriales \n",
    "#con mayor número de víctimas, por lo que iteramos, la seleccionamos y asignamos a una variable nueva \n",
    "\n",
    "    killer_chart = clean_killers_info[0]\n",
    "    \n",
    "    return killer_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(killer_chart):\n",
    "    \"\"\"Function to clean the information that was scrapped form Wikipedia\"\"\"\n",
    "\n",
    "#Es necesario hacer una segunda limpieza para quitar los caracteres que ensucian la información y después creamos \n",
    "#una lista para guardar la información.\n",
    "\n",
    "    clean_killer_chart = [x.strip().replace(\",\", \" \").split(\"\\n\\n\") for x in killer_chart]\n",
    "\n",
    "    new_df=[]\n",
    "    for row in clean_killer_chart:\n",
    "        new_row=[]\n",
    "        for element in row:\n",
    "            new_row.append(re.sub(r'\\[\\d*]','',element))\n",
    "        new_df.append(new_row)\n",
    "\n",
    "#Una vez que la información ya está limpia, utilizamos pandas para crear un dataframe\n",
    "#Indicamos cuáles queremos que sean los nombres de las columnas \n",
    "#Eliminamos filas que no hacen sentido con la tabla y que no tienen información relevante \n",
    "\n",
    "    df_killers = pd.DataFrame(new_df, columns = new_df[0]).drop([0])\n",
    "\n",
    "    return df_killers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_index31(df_killers):\n",
    "    \"\"\"Function to correct index 31 which was not aligned with the complete DataFrame\"\"\"\n",
    "    \n",
    "#La fila 31 es la única que sale mal.\n",
    "#Nos damos cuenta que falta separar por columna '\\n' en la columna \"Name\" que es donde se quedó toda la info\n",
    "#lo asignamos a una variable para después usarla para asignar un valor \n",
    "\n",
    "    row_31 = df_killers.loc[31]['Name'].split('\\n') \n",
    "\n",
    "#Una vez que ya tenemos la info separada en una lista, iteramos sobre cada columna e índice \n",
    "#para asignar cada valor que le corresponde según la lista que creamos arriba. \n",
    "\n",
    "    for cell,column in enumerate(df_killers.columns):\n",
    "        df_killers.loc[31][column] = row_31[cell]\n",
    "        \n",
    "    return df_killers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def API_call(df_killers):\n",
    "    \"\"\"Function to extract information fro API \"Data News\" to asign a news article related to each serial killer\"\"\"\n",
    "\n",
    "#Por el nombre de cada asesino hacemos un request para almacenar el URL con la noticia relacionada al asesino.\n",
    "#Si no se encuentra una noticia para el asesino, asignamos \"No news found\". \n",
    "#Usamos la librería urllib para el encoding de URL.\n",
    "#Guardamos la información en un diccionario.\n",
    "\n",
    "    news = {}\n",
    "\n",
    "    for name in df_killers['Name']:\n",
    "        url = f'http://api.datanews.io/v1/news?q=%22{urllib.parse.quote(name)}%22'\n",
    "        info = requests.get(url, headers = {'x-api-key': '04rolxy69c1vzr6wi5xplfqyf'})\n",
    "        if info.json()['numResults'] != 0:\n",
    "            news[name]=(info.json()['hits'][0]['url'])\n",
    "        else:\n",
    "            news[name]='No news found'\n",
    "            time.sleep(2)\n",
    "\n",
    "        \n",
    "#La información que guardamos en el diccionario la pasamos a un dataframe para visualización. \n",
    "    df_news = pd.DataFrame(news, index=[0]).T.rename(columns={0: 'url'})\n",
    "    \n",
    "    return df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_data(df_killers, df_news):\n",
    "    \"\"\"Function to extract information from API \"Data News, asign a news article related to each serial killer\n",
    "    and save the output as a CSV file.\"\"\"\n",
    "\n",
    "#Unimos el dataframe que contiene la información de los asesinos seriales con la que contiene el url de la \n",
    "#noticia por asesino para consolidar los resultados.Tenemos que hacer que coincidan en el índice, por lo que en \n",
    "#df_killers pasamos la columna \"Name\" como el índice y posterior hacemos el join. \n",
    "#Guardar la información en un CSV para análises posteriores. \n",
    "\n",
    "    df_killers = df_killers.set_index(df_killers['Name'], drop=False).drop('Name',axis=1)\n",
    "    final_df = df_killers.join(df_news)\n",
    "    final_df_csv = final_df.to_csv('serial_killers2.csv', index = False)\n",
    "    \n",
    "    return final_df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_killers():\n",
    "    \n",
    "    \"\"\"Final function that englobes all the previous functions.\"\"\"\n",
    "\n",
    "    website = scrap()\n",
    "    \n",
    "    website_clean = cleaning(website)\n",
    "    \n",
    "    corrector = correction_index31(website_clean)\n",
    "    \n",
    "    API = API_call(corrector)\n",
    "    \n",
    "    results = final_data(corrector, API)\n",
    "    \n",
    "    \n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_killers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
